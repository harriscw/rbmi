---
title: "rbmi: Statistical Specifications"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    number_sections: true
    citation_package: natbib
    base_format: rmarkdown::html_vignette
bibliography: "references.bib"
link-citations: true   
linkcolor: blue
vignette: >
  %\VignetteIndexEntry{rbmi: Statistical Specifications}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Scope of this document

This document describes the statistical methods implemented in the `rbmi` R package for standard and reference-based multiple imputation of continuous longitudinal outcomes. 
The package implements two classes of multiple imputation approaches: 

1. Conventional multiple imputation methods based on Bayesian posterior draws of model parameters combined with Rubin's rule to make inferences as described in @CarpenterEtAl2013 and @CroEtAlTutorial2020.

2. Conditional mean imputation methods combined with re-sampling techniques as described in @Wolbers2021. 

The document is structured as follows: we first provide an informal introduction to estimands and corresponding treatment effect estimation based on multiple imputation (section \@ref(sec:intro)). The core of this document consists of section \@ref(sec:statsMethods) which describes the statistical methodology in detail. The link between theory and the functions included in package `rbmi` is provided in section \@ref(sec:rbmiFunctions). We conclude with a comparison of our package to some alternative software implementations of reference-based imputation methods (section \@ref(sec:otherSoftware)).

# Introduction to estimands and estimation methods {#sec:intro}

## Estimands

The ICH E9(R1) addendum on estimands and sensitivity analyses is a systematic approach to ensure alignment among clinical trial objectives, trial execution/conduct, statistical analyses, and interpretation of results [@iche9r1]. 
As per the addendum, an estimand is a precise description of the treatment effect reflecting the clinical question posed by the trial objective which summarizes at a population-level what the outcomes would be in the same patients under different
treatment conditions being compared.
One important attribute of an estimand is a list of possible intercurrent events (ICEs), i.e. of events occurring after treatment initiation that affect either the interpretation or the existence of the measurements associated with the clinical question of interest, and the definition of appropriate strategies to deal with ICEs. The three most relevant strategies for the purpose of this document are the hypothetical strategy, the treatment policy strategy, and the composite strategy. For the hypothetical strategy, a scenario is envisaged in which the ICE would not occur. Under this scenario, endpoint values after the ICE are not directly observable and treated using models for missing data. For the treatment policy strategy, the occurrence of the ICE is considered irrelevant in defining the treatment effect of interest and the value for the outcome of interest is used regardless of whether or not the
ICE occurs. For the composite strategy, the ICE itself is included as a component of the endpoint.

## Alignment between the estimand and the estimation method 

The ICH E9(R1) addendum distinguishes between ICEs and missing data [@iche9r1]. Whereas ICEs such as treatment discontinuations reflect clinical practice, the amount of missing data can be minimized in the conduct of a clinical trial. However, there are many connections between missing data and ICEs. For example, it is often difficult to retain subjects in a clinical trial after treatment discontinuation and a subject's withdrawal from the trial leads to missing data. As another example, for an ICE handled by a hypothetical strategy, outcome values after the ICE are not directly observable under the hypothetical scenario. Consequently, any observed outcome values after such ICEs are typically discarded and treated as missing data. 

The addendum proposes that estimation methods to address the problem presented by missing data should be selected to align with the estimand. A recent overview of methods to align the estimator with the estimand is @Mallinckrodt2020. A short introduction on estimation methods for studies with longitudinal endpoints can also be found in @Wolbers2021. One prominent statistical method for this purpose is multiple imputation (MI), which is the target of the `rbmi` package. 

### Missing data prior to ICEs

Missing data may also occur in subjects without an ICE or prior to the occurrence of an ICE. As such missing outcomes are not associated with an ICE, it is often plausible to impute them under a missing-at-random (MAR) assumption. Informally, MAR occurs if the missing data can be fully accounted for by the baseline variables included in the model and the observed longitudinal outcomes, and if the model is correctly specified. MAR informally states that unobserved values after an ICE would have been similar to the observed data from subjects who did not have the ICE and remained under follow-up.  However, in some situations, it may be more reasonable to assume that missingness is "informative" and indicates a systematically better or worse outcome than in observed subjects. In such situations, MNAR imputation with a $\delta$-adjustment could be explored as sensitivity analyses. $\delta$-adjustments add a fixed or random quantity to the imputations in order to make the imputed outcomes systematically worse or better than those observed. $\delta$-adjustments are also described in @CroEtAlTutorial2020.

### Implementation of the hypothetical strategy

The MAR assumption is often a good starting point for implementing a hypothetical strategy. MNAR imputation with a $\delta$-adjustment may also be used for sensitivity analyses [@Mallinckrodt2020].

### Implementation of the treatment policy strategy

Ideally, data collection continues after an ICE handled with a treatment policy strategy and no missing data arises. 
Indeed, such post-ICE data are increasingly systematically collected in RCTs. 
However, despite best efforts, missing data after an ICE such as study treatment discontinuation may still occur because the subject drops out from the study after discontinuation. It is difficult to give definite recommendations regarding the implementation of the treatment policy strategy in the presence of missing data at this stage because the optimal method is highly context dependent and a topic of ongoing statistical research.

For ICEs which are thought to have a negligible effect on efficacy outcomes, standard MAR-based imputation may be appropriate. In contrast, an ICE such as treatment discontinuation may be expected to have a more substantial impact on efficacy outcomes. In such settings, the MAR assumption may still be plausible after conditioning on the subject's time-varying treatment status [@Guizzaro2021]. In this case, one option is to impute missing post-discontinuation data based on subjects who also discontinued treatment but continued to be followed up [@PolverejanDragalin2020]. Another option which may require somewhat less post-discontinuation data is to include all subjects in the imputation procedure but to model post-discontinuation data by using a time-varying treatment status indicators (e.g. time-varying indicators of treatment compliance, discontinuation, or initiation of rescue
treatment)) [@Guizzaro2021]. In this approach, post-ICE outcomes are included
in every step of the analysis, including in the fitting of the imputation model. 
It assumes that ICEs may impact post-ICE outcomes but that otherwise missingness is non-informative. The approach also assumes that the time-varying covariates do not contain missing values, deviations in outcomes after the ICE are correctly modeled by these time varying covariates, and that sufficient post-ICE data are available to reliably estimate the effect of ICEs on outcome data. These proposals are relatively recent and there remain open questions regarding the appropriate trade-off between model complexity (e.g. to account for a differential effect depending on the timing of the treatment discontinuation) and the variance in the resulting treatment effect estimate. More generally, it is not yet established how much post-discontinuation data is required to implement such methods robustly and without the risk of substantial inflation of variance.    

In practice, treatment discontinuation rates may be relatively low or it may be difficult to retain patients in the trial after the ICE. In such settings, the amount of available post treatment discontinuation data may be insufficient to inform the imputation model. Depending on the disease area and the anticipated mechanism of action of the intervention, it may be plausible to assume that subjects in the intervention group behave similarly to subjects in the control group after the ICE treatment discontinuation. In this case, reference-based imputation methods are an option [@Mallinckrodt2020]. Reference-based imputation methods are increasingly popular for handling missing data under a missing-not-at-random (MNAR) assumption. They formalize the idea to impute missing data in the intervention group based on data from a control or reference group. For a general description and review of reference-based imputation methods, we refer to @CarpenterEtAl2013, @CroEtAlTutorial2020, and @Wolbers2021. For a technical description of the implemented statistical methodology for reference-based imputation, we refer to section \@ref(sec:statsMethods) (in particular section \@ref(sec:imputationStep)). 

### Implementation of the composite strategy

The composite strategy is typically applied to binary outcomes but it can also be used for continuous outcomes by ascribing a suitably unfavorable value to patients who experience ICEs for which a composite strategy has been defined. As described in @Darken2020, this could be implemented using MI with a $\delta$-adjustment for post-ICE data.

# Statistical methodology {#sec:statsMethods}

## Overview of the imputation procedure {#sec:methodsOverview}

Analyses of datasets with missing data always rely on missing data assumptions. The methods described here can be used to produce valid imputations under a MAR assumption or under reference-based imputation assumptions. MNAR imputation based on fixed $\delta$-adjustments as typically used in sensitivity analyses such as tipping-point analyses are also supported. 

Two general imputation approaches are implemented in `rbmi`:

1. Conventional multiple imputation based on Bayesian (or approximate Bayesian) posterior draws from the imputation model combined with Rubin's rule for inference.

2. Conditional mean imputation based on the REML estimate of the imputation model combined with resampling techniques for inference. 

The **multiple imputation** approach includes the following steps:

1. **Base imputation model fitting step** (Section \@ref(sec:imputationModel))

  + Fit a Bayesian multivariate normal/ mixed model for repeated measures (MMRM) model to the observed longitudinal outcomes after exclusion of data after ICEs for which reference-based missing data imputation is desired (Section \@ref(sec:imputationModelBayes)). Draw $M$ posterior samples of the estimated parameters (regression coefficients and covariance matrices) from this model. 
  
  + Alternatively, $M$ approximate posterior draws from the posterior distribution can be sampled by repeatedly applying conventional restricted maximum-likelihood (REML) parameter estimation of the MMRM model to nonparametric bootstrap samples from the original dataset (Section \@ref(sec:imputationModelBoot)). 
  
2. **Imputation step** (Section \@ref(sec:imputationStep))

  + Take a single sample $m$ ($m\in 1,\ldots, M)$ from the posterior distribution of the imputation model parameters.

  + For each subject, use the sampled parameters and the defined strategy for dealing with their ICEs to determine the mean and covariance matrix describing the subject's marginal outcome distribution for all longitudinal outcome assessments (i.e. observed and missing outcomes). 

  + For each subjects, construct the conditional multivariate normal distribution of their missing outcomes given their observed outcomes (including observed outcomes after ICEs for which reference-based missing data imputation is desired). 
  
  + For each subject, draw a single sample from this conditional distributions to impute their missing outcomes leading to a complete imputed dataset.
  
  + For sensitivity analyses, a pre-defined $\delta$-adjustment may be applied to the imputed data prior to the analysis step. (Section \@ref(sec:deltaAdjustment)).

3. **Analysis step** (Section \@ref(sec:analysis))

  + Analyze the imputed dataset using an analysis model (e.g. ANCOVA) resulting in a point estimate and a standard error (with corresponding degrees of freedom) of the targeted treatment effect. 
  
4. **Pooling step for inference** (Section \@ref(sec:pooling))

  + Repeat steps 2. and 3. for each posterior sample $m$, resulting in $M$ complete datasets, $M$ point estimates of the treatment effect, and $M$ standard errors (with corresponding degrees of freedom). Pool the $M$ treatment effect estimates, standard errors, and degrees of freedom using the rules by Barnard and Rubin to obtain the final pooled treatment effect estimator, standard error, and degrees of freedom. 

The **conditional mean imputation** approach includes the following steps:

1. **Base imputation model fitting step** (Section \@ref(sec:imputationModel))

  + Fit a conventional multivariate normal/MMRM model using restricted maximum likelihood (REML) to the observed longitudinal outcomes after exclusion of data after ICEs for which reference-based missing data imputation is desired (Section \@ref(sec:imputationModelREML)).
  
2. **Imputation step** (Section \@ref(sec:imputationStep))

  + For each subject, use the fitted parameters from step 1. to construct the conditional distribution of missing outcomes given observed outcomes (including observed outcomes after ICEs for which reference-based missing data imputation is desired) as described above.
  
  + For each subject, impute their missing data deterministically by the conditional mean of this conditional distribution leading to a complete imputed dataset.
  
  + For sensitivity analyses, a pre-defined $\delta$-adjustment may be applied to the imputed data prior to the analysis step. (Section \@ref(sec:deltaAdjustment)).
  
3. **Analysis step** (Section \@ref(sec:analysis))

  + Apply an analysis model (e.g. ANCOVA) to the completed dataset resulting in a point estimate of the targeted treatment effect. 
  
4. **Jackknife or bootstrap inference step** (Section \@ref(sec:bootInference))

  + Inference for the treatment effect estimate from 3. is based on re-sampling techniques. Both the jackknife and the bootstrap are supported.

## Setting, notation, and missing data assumptions

Assume that the data are from a study with $n$ subjects in total and that each subject $i$ ($i=1,\ldots,n$) has $J$ scheduled follow-up visits at which the outcome of interest is assessed. 
In most applications, the data will be from a randomized trial of an intervention vs a control group and the treatment effect of interest is a comparison in outcomes at a specific visit between these randomized groups. However, single-arm trials or multi-arm trials are in principle also supported by the implementation. 

Denote the observed outcome vector of length $J$ for subject $i$ by $Y_i$ (with missing assessments coded as NA (not available)) and its non-missing and missing components by $Y_{i!}$ and $Y_{i?}$, respectively. 
By default, imputation of all outcomes in $Y_{i}$ is performed under a MAR assumption. Therefore, if missing data following an ICE are to be handled using MAR imputation, this is compatible with the default assumption. As discussed in the Section \@ref(sec:intro), the MAR assumption is often a good starting point for implementing a hypothetical strategy. But also note that observed outcome data after an ICE handled using a hypothetical strategy is not compatible with this strategy. Therefore, we assume that all post ICE data after ICEs handled using a hypothetical strategy are already set to NA in $Y_i$. However, any observed outcomes after ICEs handled using a treatment policy strategy should be included in $Y_i$ as they are compatible with this strategy. 

Subjects may also experience up to one ICE after which missing data imputation according to a reference-based imputation method is foreseen. For a subject $i$ with such an ICE, denote their first visit which is affected by the ICE by $\tilde{t}_i \in \{1,\ldots,J\}$. For all other subjects, set $\tilde{t}_i=\infty$. A subject's outcome vector after setting observed outcomes from visit $\tilde{t}_i$ onwards to missing (i.e. NA) is denoted as $Y'_i$ and the corresponding data vector after removal of NA elements as $Y'_{i!}$.

MNAR $\delta$-adjustments are added to the imputed datasets after the formal imputation steps and is covered in a separate section (Section \@ref(sec:deltaAdjustment)).

## The base imputation model {#sec:imputationModel}

### Model specification {#sec:imputationModelSpecs}

The purpose of the imputation model is to estimate (covariate-dependent) mean trajectories and covariance matrices for each group in the absence of ICEs handled using reference-based imputation methods. Conventionally,
publications on reference-based imputation methods have implicitly assumed that the corresponding post-ICE
data is missing for all subjects [@CarpenterEtAl2013]. We also allow the situation where post-ICE data
is available for some subjects but needs to be imputed using reference-based methods for others. However,
any observed data after ICEs for which reference-based imputation methods are specified are not compatible
with the imputation model described below and they are therefore removed and considered as missing for
the purpose of estimating the imputation model, and for this purpose only. 
That is, the base imputation model is fitted to $Y'_{i!}$ and not to $Y_{i!}$. 
If we did not exclude these data, then the imputation model would mistakenly estimate mean trajectories based on a mixture of observed pre- and post-ICE data which are not relevant for reference-based imputations.

The base imputation model of the longitudinal outcomes $Y'_i$ assumes that the mean structure is a linear function of covariates. Full flexibility for the specification of the linear predictor of the model is supported. At a minimum, the covariates should include the treatment group, the (categorical) visit, and treatment-by-visit interactions. Typically, other covariates including the baseline outcome are also included.
External time-varying covariates (e.g. calendar time of the visit) as well as internal time-varying (e.g. time-varying indicators of treatment discontinuation or initiation of rescue treatment) may in principle also be included if indicated [@Guizzaro2021]. Missing covariate values are not allowed. This means that the values of time-varying covariates must be non-missing at every visit regardless of whether the outcome is measured or missing. 

Denote the $J\times p$ design matrix for subject $i$ corresponding to the mean structure model by $X_i$ and the same matrix after removal of rows corresponding to missing outcomes in $Y'_{i!}$ by $X'_{i!}$. 
Here $p$ is the number of parameters in the mean structure of the model for the elements of $Y'_{i!}$.
The base imputation model for the observed outcomes is defined as:
$$ Y'_{i!} = X'_{i!}\beta + \epsilon_{i!} \mbox{ with } \epsilon_{i!}\sim N(0,\Sigma_{i!!})$$
where $\beta$ is the vector of regression coefficients and $\Sigma_{i!!}$ is a covariance matrix which is obtained from the complete-data $J\times J$-covariance matrix $\Sigma$ by omitting rows and columns corresponding to missing outcome assessments for subject $i$. 

Typically, a common unstructured covariance matrix for all subjects is assumed for $\Sigma$ but separate covariate matrices per treatment group are also supported. Indeed, the implementation also supports the specification of separate covariate matrices according to an arbitrarily defined categorical variable which groups the subjects into disjoint subset. For example, this could be useful if different covariance matrices are suspected in different subject strata. Finally, for all imputation methods described below that do not rely on Bayesian model fitting, there is further flexibility in the choice of the covariance structure, i.e. unstructured (default), heterogeneous Toeplitz, heterogeneous compound symmetry, and AR(1) covariance structures are supported. 

### Restricted maximum likelihood estimation (REML) {#sec:imputationModelREML}

Frequentist parameter estimation for the base imputation is based on REML. The use of REML as an improved alternative to maximum likelihood for covariance parameter estimation was originally proposed by @Patterson1971. Since then, it has become the default method for parameter estimation in linear mixed effects models.

### Bayesian model fitting {#sec:imputationModelBayes}

The Bayesian imputation model is fitted with the R package `rstan` [@Rstan]. `rstan` is the R interface of Stan. Stan is a powerful and flexible statistical software developed by a dedicated team and implements Bayesian inference with state-of-the-art MCMC sampling procedures. The multivariate normal model with missing data specified in section \@ref(sec:imputationModelSpecs) can be considered a generalization of the models described in the Stan user's guide (see @Rstan [section 3.5]).

The same prior distributions as in the SAS implementation of the "five macros" are used [@FiveMacros], i.e. an improper flat priors for the regression coefficients and a weakly informative inverse Wishart prior for the covariance matrix (or matrices). Specifically, let $S \in \mathbb{R}^{J \times J}$ be a symmetric positive definite matrix and $\nu \in (J-1, \infty)$. Then the symmetric positive definite matrix $x \in \mathbb{R}^{J \times J}$ has density:
$$
\text{InvWish}(x \vert \nu, S) = \frac{1}{2^{\nu J/2}} \frac{1}{\Gamma_J(\frac{\nu}{2})} \vert S \vert^{\nu/2} \vert x \vert ^{-(\nu + J + 1)/2} \text{exp}(-\frac{1}{2} \text{tr}(Sx^{-1})).
$$
For $\nu > J+1$ the mean is given by: 
$$
E[x] = \frac{S}{\nu - J - 1}.
$$
We choose $S$ equal to the estimated covariance matrix from the frequentist REML fit and $\nu = J+2$ as these are the lowest degrees of freedom that guarantee a finite mean. Setting the degrees of freedom with such a low $\nu$ ensures that the prior has little impact on the posterior. Moreover, this choice allows to interpret the parameter $S$ as the mean of the prior distribution.

As in the "SAS five macros", the MCMC algorithm is initialized at the parameters from a frequentist REML fit (see section \@ref(sec:imputationModelREML)). As described above, we are using only weakly informative priors for the parameters. Therefore, the Markov chain is essentially starting from the targeted stationary posterior distribution and only a minimal amount of burn-in of the chain is required. 

### Approximate Bayesian posterior draws via the bootstrap {#sec:imputationModelBoot}

Several authors have suggested that a faster and stabler way to get Bayesian posterior draws from the imputation model is to bootstrap the incomplete data and to calculate REML estimates for each bootstrap sample (@LittleRubin1992, @Efron1994, @Honaker2010, @vanHippelBartlett2021). This method is proper in that the REML estimates from the bootstrap samples are asymptotically equivalent to a sample from the posterior distribution and may provide additional robustness to model misspecification (@LittleRubin1992 [Section 10.2.3, part 6], @Honaker2010). In order to retain balance between treatment groups and stratification factors across bootstrap samples, the user is able to provide stratification variables for the bootstrap in the `rbmi` implementation.

## Imputation step {#sec:imputationStep}

### Marginal imputation distribution for a subject - MAR case  {#sec:imputatioMNAR}

For each subject $i$, the marginal distribution of the complete $J$-dimensional outcome vector from all assessment visits according to the imputation model is a multivariate normal distribution . Its mean $\tilde{\mu}_i$ is given by the predicted mean from the imputation model conditional on the subject's baseline characteristics, group, and, optionally, time-varying covariates. Its covariance matrix $\tilde{\Sigma}_i$ is given by the overall estimated covariance matrix or, if different covariance matrices are assumed for different groups, the covariance matrix corresponding to the subject $i$'s group.  

### Marginal imputation distribution for a subject - reference-based imputation methods {#sec:imputationRefBased}

For each subject $i$, we calculate the mean and covariance matrix of the complete $J$-dimensional outcome vector from all assessment visits as for the MAR case and denote them by $\mu_i$ and $\Sigma_i$.
For reference-based imputation methods, a corresponding reference group is also required for each group. Typically, the reference group for the intervention group will be the control group. 
The reference mean $\mu_{ref,i}$ is defined as the predicted mean from the imputation model conditional on the reference group (rather than the actual group subject $i$ belongs to) and the subject's baseline characteristics. 
The reference covariance matrix $\Sigma_{ref,i}$ is the overall estimated covariance matrix or, if different covariance matrices are assumed for different groups, the estimated covariance matrix corresponding to the reference group. In principle, time-varying covariates could also be included in reference-based imputation methods. However, this is only sensible for external time-varying covariates (e.g. calendar time of the visit) and not for internal time-varying covariates (e.g. treatment discontinuation) because the latter likely depend on the actual treatment group and it is typically not sensible to assume the same trajectory of the time-varying covariate for the reference group. 

Based on these means and covariance matrices, the subject's marginal imputation distribution for the reference-based imputation methods is then calculated as detailed in @CarpenterEtAl2013 [Section 4.3]. 
Denote the mean and covariance matrix of this marginal imputation distribution by $\tilde{\mu}_i$ and $\tilde{\Sigma}_i$. Recall that the subject's first visit which is affected by the ICE is denoted by $\tilde{t}_i \in \{1,\ldots,J\}$ (and visit $\tilde{t}_i-1$ is the last visit unaffected by the ICE). The marginal distribution for the patient $i$ is then built according to the specific assumption for the data up to and post the ICE as follows:

1. Jump to reference (JR): the patient's outcome distribution is normally distributed with the following mean:
$$\tilde{\mu}_i = (\mu_i[1], \dots, \mu_i[\tilde{t}_i-1], \mu_{ref,i}[\tilde{t}_i], \dots, \mu_{ref,i}[J])^T.$$
The covariance matrix is constructed as follows. First, we partition the covariance matrices $\Sigma_i$ and $\Sigma_{ref,i}$ in blocks according to the time of the ICE $\tilde{t}_i$:
$$
\Sigma_{i} = \begin{bmatrix} \Sigma_{i, 11} & \Sigma_{i, 12} \\
\Sigma_{i, 21} & \Sigma_{i,22} \\
\end{bmatrix}
$$
$$
\Sigma_{ref,i} = \begin{bmatrix} \Sigma_{ref, i, 11} & \Sigma_{ref, i, 12} \\
\Sigma_{ref, i, 21} & \Sigma_{ref, i,22} \\
\end{bmatrix}.
$$
We want the covariance matrix $\tilde{\Sigma}_i$ to match $\Sigma_i$ for the pre-deviation measurements, and $\Sigma_{ref,i}$ for the conditional components for the post-deviation given the pre-deviation measurements. The solution is derived in @CarpenterEtAl2013 [section 4.3] and is given by:
$$
\begin{matrix}
\tilde{\Sigma}_{i,11} = \Sigma_{i, 11} \\
\tilde{\Sigma}_{i, 21} = \Sigma_{ref,i, 21} \Sigma^{-1}_{ref,i, 11} \Sigma_{i, 11} \\
\tilde{\Sigma}_{i, 22} = \Sigma_{ref, i, 22} - \Sigma_{ref,i, 21} \Sigma^{-1}_{ref,i, 11} (\Sigma_{ref,i, 11} - \Sigma_{i,11}) \Sigma^{-1}_{ref,i, 11} \Sigma_{ref,i, 12}.
\end{matrix}
$$

2. Copy increments in reference (CIR): the patient's outcome distribution is normally distributed with the following mean:
$$
\tilde{\mu}_i = (\mu_i[1], \dots, \mu_i[\tilde{t}_i-1], \mu_i[\tilde{t}_i-1] + (\mu_{ref,i}[\tilde{t}_i] - \mu_{ref,i}[\tilde{t}_i-1]), \dots,
\mu_i[\tilde{t}_i-1]+(\mu_{ref,i}[J] - \mu_{ref,i}[\tilde{t}_i-1]))^T.
$$
The covariance matrix is derived as in the JR method.

3. Copy reference (CR): the patient's outcome distribution is normally distributed with mean and covariance matrix taken from the reference group:
$$
\tilde{\mu}_i = \mu_{ref,i}
$$
$$
\tilde{\Sigma}_i = \Sigma_{ref,i}.
$$

4. Last mean carried forward (LMCF): the patient's outcome distribution is normally distributed with the following mean:
$$ \tilde{\mu}_i = (\mu_i[1], \dots, \mu_i[\tilde{t}_i-1], \mu_i[\tilde{t}_i-1], \dots, \mu_i[\tilde{t}_i-1])'$$
and covariance matrix: $$ \tilde{\Sigma}_i = \Sigma_i.$$

### Imputation of missing outcome data {#sec:imputationRandomConditionalMean}

The joint marginal multivariate normal imputation distribution of subject $i$'s observed and missing outcome data has mean $\tilde{\mu}_i$ and covariance matrix $\tilde{\Sigma}_i$ as defined above. The actual imputation of the missing outcome data is obtained by conditioning this marginal distribution on the subject's observed outcome data. Of note, this approach is valid regardless whether the subject has intermittent or terminal missing data. 

The conditional distribution used for the imputation is again a multivariate normal distribution and explicit formulas for the conditional mean and covariance are readily available. For completeness, we report them here with the notation and terminology of our setting. The marginal distribution for the outcome of patient $i$ is $Y_i \sim N(\tilde{\mu}_i, \tilde{\Sigma}_i)$ and the outcome $Y_i$ can be decomposed in the observed ($Y_{i,!}$) and the unobserved ($Y_{i,?}$) components. Analogously the mean $\tilde{\mu}_i$ can be decomposed as $(\tilde{\mu}_{i,!},\tilde{\mu}_{i,?})$ and the covariance $\tilde{\Sigma}_i$ as:
$$
\tilde{\Sigma}_i = 
\begin{bmatrix}
\tilde{\Sigma}_{i, !!} & \tilde{\Sigma}_{i,!?} \\
\tilde{\Sigma}_{i, ?!} & \tilde{\Sigma}_{i, ??}
\end{bmatrix}.
$$
The conditional distribution of $Y_{i,?}$ conditional on $Y_{i,!}$ is then a multivariate normal distribution with expectation
$$
E(Y_{i,?} \vert Y_{i,!})= \tilde{\mu}_{i,?} + \tilde{\Sigma}_{i, ?!} \tilde{\Sigma}_{i,!!}^{-1} (Y_{i,!} - \tilde{\mu}_{i,!})
$$
and covariance matrix
$$
Cov(Y_{i,?} \vert Y_{i,!}) = \tilde{\Sigma}_{i,??} - \tilde{\Sigma}_{i,?!} \tilde{\Sigma}_{i,!!}^{-1} \tilde{\Sigma}_{i,!?}.
$$

Conventional random imputation consists in sampling from this conditional multivariate normal distribution. Conditional mean imputation imputes missing values with the deterministic conditional expectation $E(Y_{i,?} \vert Y_{i,!})$.

## $\delta$-adjustment {#sec:deltaAdjustment}

A *marginal* $\delta$-adjustment approach similar to the "five macros" in SAS is implemented [@FiveMacros], i.e. fixed non-stochastic values are added after the multivariate normal imputation step and prior to the analysis.
This is relevant for sensitivity analyses in order to make imputed data systematically worse or better, respectively, than observed data. In addition, some authors have suggested $\delta$-type adjustments to implement a composite strategy for continuous outcomes [@Darken2020].

The implementation provides full flexibility regarding the specific implementation of the $\delta$-adjustment, i.e. the value that is added may depend on the randomized treatment arm, the timing of the subject's ICE, and other factors. For suggestions and case studies regarding this topic, we refer to @CroEtAlTutorial2020.


## Analysis step {#sec:analysis}

After data imputation, a standard analysis model can be applied to the completed data resulting in a treatment effect estimate. As the imputed data no longer contains missing values, the analysis model is often simple. For example, it can be an analysis of covariance (ANCOVA) model with the outcome (or the change in the outcome from baseline) at a specific visit j as the dependent variable, the randomized treatment group as the primary covariate and, typically, adjustment for the same baseline covariates as for the imputation model.

## Pooling step for inference of (approximate) Bayesian multiple imputations and the Barnard and Rubin rules {#sec:pooling}

Assume that the analysis model has been applied to $M$ multiple imputed random datasets which resulted in $m$ treatment effect estimates $\hat{\theta}_m$ ($m=1,\ldots,M$) with corresponding standard error $SE_m$ and (if available) degrees of freedom $\nu_{com}$. If degrees of freedom are not available for an analysis model, set $\nu_{com}=\infty$ for inference based on the normal distribution. 

Rubin's rules are used for pooling the treatment effect estimates and corresponding variances estimates from the analysis steps across the $M$ multiple imputed datasets. According to Rubin's rules, the final estimate of the treatment effect is calculated as the sample mean over the $M$ treatment effect estimates:
$$
\hat{\theta} = \frac{1}{M} \sum_{m = 1}^M \hat{\theta}_m.
$$
The pooled variance is based on two components that reflect the within and the between variance of the treatment effects across the multiple imputed datasets:
$$
V(\hat{\theta}) = V_W(\hat{\theta}) + (1 + \frac{1}{M}) V_B(\hat{\theta})
$$
where $V_W(\hat{\theta}) = \frac{1}{M}\sum_{m = 1}^M SE^2_m$ is the within-variance and $V_B(\hat{\theta}) = \frac{1}{M-1} \sum_{m = 1}^M (\hat{\theta}_m - \hat{\theta})^2$ is the between-variance.

Confidence intervals and tests of the null hypothesis $H_0: \theta=\theta_0$ are based on the $t$-statistics $T$:

$$ T= (\hat{\theta}-\theta_0)/\sqrt{V(\hat{\theta})}. $$
Under the null hypothesis, $T$ has an approximate $t$-distribution with $\nu$ degrees of freedom. $\nu$ is calculated according to the Barnard and Rubin approximation [@Barnard1999 (formula 3), @LittleRubin1992 (formula (5.24), page 87)]: 

$$
\nu = \frac{\nu_{old}* \nu_{obs}}{\nu_{old} + \nu_{obs}}
$$
with
$$
\nu_{old} = \frac{M-1}{\lambda^2} \quad\mbox{and}\quad \nu_{obs} = \frac{\nu_{com} + 1}{\nu_{com} + 3} \nu_{com} (1 - \lambda)
$$
where $\lambda = \frac{(1 + \frac{1}{M})V_B}{V(\hat{\theta})}$ is the fraction of missing information.

## Boostrap and jackknife inference for conditional mean imputation {#sec:bootInference}

Several authors have proposed to use bootstrap inference for consistent standard error estimation for multiple imputation methods under uncongeniality and model misspecification, see @BartlettHughes2020 for a review. Specifically, bootstrap inference for multiple imputation methods based on maximum likelihood estimation of the imputation parameters (rather than random sampling from the Bayesian posterior distribution) is justified and discussed in @vanHippelBartlett2021. As an alternative to the bootstrap, the jackknife is also implemented.

These alternative methods are particularly relevant here because reference-based imputation methods are not congenial and, consequently, Rubin's rule may not provide valid frequentist inference [@Seaman2014, @LiuPang2016, @Tang2017, @Bartlett2021]. We revisit this topic in Section  \@ref(sec:methodsComparison). 

### Point estimate of the treatment effect

The point estimator is obtained by applying the analysis model (Section \@ref(sec:analysis)) to a single conditional mean imputation of the missing data (see Section \@ref(sec:imputationRandomConditionalMean)) based on the REML estimator of the parameters of the imputation model (see Section \@ref(sec:imputationModelREML)). We denote this treatment effect estimator by $\hat{\theta}_{MLCMI}$.

@vanHippelBartlett2021 propose to perform $M$ random imputations based on the REML estimator instead, to analyse them, and to use the average of treatment effects estimates across random imputations as the overall treatment effect estimator. We denote the treatment effect from a single randomly imputed dataset $m$ by $\hat{\theta}_{MLSI,m}$ and the overall average treatment effect by $\hat{\theta}_{MLMI,M}=\frac{1}{M}\sum_{m=1}^M \hat{\theta}_{MLSI,m}$ They also state that using the maximum likelihood estimator (or the asymptotically equivalent REML estimator) leads to a slightly more efficient estimator than using Bayesian posterior draws. 

For multivariate normal imputation and linear regression, e.g. ANCOVA, as the analysis model, it is easy to show that the proposed estimator $\hat{\theta}_{MLCMI}$ based on a single imputed dataset using conditional mean imputation is identical to the pooled treatment effect under multiple random imputation with an infinity number of imputation or, more precisely, $$ \hat{\theta}_{MLMI,M} \to \hat{\theta}_{MLCMI} \mbox{  as  } M\to\infty \mbox{ (a.s.)}$$
Thus, in our specific setting, the proposed estimator corresponds to an efficient implementation of the proposal by @vanHippelBartlett2021. For a simple proof of this fact, we refer to @Wolbers2021. We expect that conditional mean imputation is also valid for general MMRM analysis models but this has not been formally justified.

### Bootstrap confidence intervals (CI) and tests for the treatment effect

@vanHippelBartlett2021 discuss analytic formulas of standard errors for random maximum likelihood multiple imputations but these formulas are not straightforward to translate to our setting. Therefore, we propose to use resampling techniques for conducting frequentist inference for REML conditional mean imputation. As in section \@ref(sec:imputationModelBoot), we also allow the user to use the stratified bootstrap in order to retain balance between treatment groups and stratification factors across bootstrap samples.

Two different bootstrap methods are implemented in `rbmi`: Methods based on the *bootstrap standard error and the normal approximation* and *percentile bootstrap methods*. Denote the treatment effect estimates from $B$ bootstrap samples by $\hat{\theta}^*_b$ ($b=1,\ldots,B$). The *bootstrap standard error* $\hat{se}_{boot}$ is defined as the empirical standard deviation of the bootstrapped treatment effect estimates. The corresponding two-sided normal approximation $1-\alpha$ CI is defined as $\hat{\theta}\pm z^{1-\alpha/2}\cdot \hat{se}_{boot}$ where $\hat{\theta}$ is the treatment effect estimate in the original dataset, i.e. $\hat{\theta}=\hat{\theta}_{CMI}$. Tests of the null hypothesis $H_0: \theta=\theta_0$ are then based on the $Z$-score $Z=(\hat{\theta}-\theta_0)/\hat{se}_{boot}$ using a standard normal approximation. We refer to @EfronTibs1994 and @DavisonHinkley1997 for general background on bootstrap methods and, in particular, formulas for the percentile bootstrap. Explicit formulas for bootstrap inference as implemented in the `rbmi` package and some considerations regarding the required number of bootstrap samples are included in the Appendix of @Wolbers2021.

### Jackknife standard errors, confidence intervals (CI) and tests for the treatment effect

The jacknife is an alternative to the bootstrap and has close similarities to it [@EfronTibs1994, chapter 10]. For a sample size of $n$, the jackknife standard error depends on treatment effect estimates $\hat{\theta}_{(-b)}$ ($b=1,\ldots,n$) from samples from the original dataset which leave out observation $b$. Then, the *jackknife standard error* is defined as 
$$\hat{se}_{jack}=[\frac{(n-1)}{n}\cdot\sum_{b=1}^{n} (\hat{\theta}_{(-b)}-\bar{\theta}_{(.)})^2]^{1/2}$$ 
where $\bar{\theta}_{(.)}$ denotes the mean of all jackknife estimates. Confidence and statistical tests can then be based on the jackknife standard error as described previously for the bootstrap standard error. 

## Comparison between the implemented approaches  {#sec:methodsComparison}

The proposed Bayesian multiple imputation approaches use Rubin's combination rules for inference. 
For MAR imputation, Rubin's rule provides frequentist consistent estimates of the standard error. However, it is well known that Rubin's rule does not provide frequentist consistent estimates of the standard error for reference-based imputation methods [@Seaman2014, @LiuPang2016, @Tang2017, @CroEtAl2019, @Bartlett2021]. 
Simulations in @Wolbers2021 which relied on the `rbmi` package also confirmed this finding and demonstrated that inference based on Rubin's rule gave  inflated standard errors relative to the true repeated sampling variance, very conservative observed type I error rates and a substantially decreased statistical power for reference-based imputation methods. Intuitively, this occurs because reference-based imputation methods borrow information from the reference group for imputation in the intervention group. This creates a positive correlation between outcomes in the two treatment groups and leads to a reduction in the frequentist variance of the resulting treatment effect contrast which is not captured by Rubin's variance estimator. Formally, the discrepancy is due to uncongeniality between the imputation and analysis models in reference-based imputation methods [@Meng1994, @Bartlett2021].
@CroEtAl2019 argued that Rubin's rule is nevertheless valid for reference-based imputation methods because it is approximately information-anchored, i.e. standard error estimates from reference-based imputation methods are similar to those observed under MAR imputation. In contrast, several other authors have implicitly or explicitly favored inference that is correct from a frequentist repeated-sampling perspective [@Seaman2014, @LiuPang2016, @Tang2017, @Bartlett2021]. To us, information anchoring is a sensible concept for sensitivity analyses, whereas for a primary analyses, we feel that that it is more important to adhere to the principles of frequentist inference. 

In contrast, the proposed conditional mean imputation approach uses the jackknife or the bootstrap for inference.
This approach provides consistent point estimates with the Bayesian approach and frequentist consistent estimates of the standard error for both MAR or reference-based imputation models. 
In a simulation study based on the `rbmi` package reported in @Wolbers2021, the jackknife demonstrated exact protection of the type I error in simulations with a relatively low sample size ($n=100$ per group) and a substantial amount of missing data (>25\% of subjects with treatment discontinuations) whereas the bootstrap based on a normal approximation showed a slightly increased type I error rate compared to the nominal value (simulated rate up to 5.3\% at a nominal 5\% significance level). Based on these simulations, the jackknife is preferable to the bootstrap in our specific. Further advantages of the jackknife include that it is typically less computationally intensive and that it leads to deterministic treatment effect estimates, standard errors, and inference in this setting. This is particularly important in a regulatory setting where it is important to ascertain whether a calculated $p$-value which is close to the critical boundary of 5\% is truly below or above that threshold rather than being uncertain about this because of Monte Carlo error. 

# Mapping of statistical methods to `rbmi` functions {#sec:rbmiFunctions}

For a full documentation of the `rbmi` package functionality we refer to the help pages of all functions and the other package vignettes. Here we only give a brief overview of how the different steps of the imputation procedure are mapped to `rbmi` functions:

- The base imputation model fitting step is implemented in the function `draws()`. The chosen multiple imputation approach can be set using the argument `method` and should be one of the following:
  + Bayesian posterior parameter draws from the imputation model are obtained via the argument `method = method_bayes()`.
  + Approximate Bayesian posterior parameter draws from the imputation model are obtained via argument `method = method_approxbayes()`.
  + ML or REML parameter estimates of the imputation model parameters for the original dataset and all leave-one-subject-out datasets (as required for the jackknife) are obtained via argument `method = method_condmean(type = "jackknife")`.
  + ML or REML parameter estimates of the imputation model parameters for the original dataset and bootstrapped datasets are obtained via argument `method = method_condmean(type = "bootstrap")`.
- Random imputations based on (approximate) Bayesian posterior parameter draws and deterministic conditional mean imputation are implemented in function `impute()`. Imputation can be performed assuming the already implemented imputation strategies as presented in section \@ref(sec:imputationStep). Additionally, user-defined imputation strategies can also be provided by the user.
- The analysis step is implemented in function `analyse()` which applies the analysis model to all imputed datasets. By default, the analysis model (argument `fun`) is the `ancova()` function but alternative analysis functions can also be provided by the user. The `analyse()` function also allows $\delta$-adjustments to the imputed datasets prior to the analysis via argument `delta`.
- The inference step is implemented in function `pool()` which pools the results across imputed datasets. The Rubin and Bernard rule is applied in case of (approximate) Bayesian multiple imputation. For conditional mean imputation,  jackknife and bootstrap (normal approximation or percentile) inference is supported. 

#  Comparison to other software implementations {#sec:otherSoftware}

An established software implementation of reference-based imputation in SAS are the so-called "five macros" by James Roger [@FiveMacros]. An alternative `R` implementation which is also currently under development is the R package `RefBasedMI`[@RefbasedMIpackage].

`rbmi` has several features which are not supported by the other implementations:

1. `rbmi` allows for the usage of data collected after an ICE. For example, suppose that we want to adopt a treatment policy strategy for the ICE "treatment discontinuation". A possible implementation of this strategy is to use the observed outcome data for subjects who remain in the study after the ICE and to use reference-based imputation in case the subject drops out. In our implementation, this is implemented by excluding observed post ICE data from the imputation model which assumes MAR missingness but including them in the analysis model. To our knowledge, this would not be possible with the other implementations. 

2. In addition to the Bayesian multiple imputation approach implemented also in the other packages, our implementation provides two alternative approaches. First, we include a bootstrap procedure with conditional mean imputation as described in section \@ref(sec:bootInference). This procedures is relevant for the reference-based imputation setting because it provides valid frequentist inference even under uncongeniality and misspecification. Second, as an alternative to the MCMC sampling of Bayesian multiple imputation, we also implement an alternative bootstrap approach for the same purpose. 

3. `RefBasedMI` fits the imputation model to data from each treatment group separately which implies covariate-treatment group interactions for all covariates for the pooled data from both treatment groups. In contrast, Roger's five macros assume a joint model including data from all the randomized groups and covariate-treatment interactions covariates are not allowed. We also chose to implement a joint model but use a flexible model for the linear predictor which may or may not include an interaction term between any covariate and the treatment group. In addition, our imputation model also allows for the inclusion of time-varying covariates.

4. In our implementation, the grouping of the subjects for the purpose of the imputation model (and the definition of the reference group) does not need to correspond to the assigned treatment groups. This provides additional flexibility for the imputation procedure. It is not clear to us whether this feature is supported by Roger's five macros or `RefBasedMI`.

5. We believe that our R-based implementation is more modular than `RefBasedMI` which should facilitate further package enhancements.

In contrast, the more general causal model introduced by @White2020causal is available in the other implementations but is currently not supported by ours. 

# References 


